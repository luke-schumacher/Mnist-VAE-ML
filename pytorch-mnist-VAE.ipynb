{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Dataset transformation\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Download and load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.encoder_fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc_mu = nn.Linear(hidden_dim2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim2, latent_dim)\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(latent_dim, hidden_dim2)\n",
    "        self.decoder_fc2 = nn.Linear(hidden_dim2, hidden_dim1)\n",
    "        self.decoder_fc3 = nn.Linear(hidden_dim1, input_dim)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.encoder_fc1(x))\n",
    "        h2 = F.relu(self.encoder_fc2(h1))\n",
    "        return self.fc_mu(h2), self.fc_logvar(h2)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.decoder_fc1(z))\n",
    "        h4 = F.relu(self.decoder_fc2(h3))\n",
    "        return torch.sigmoid(self.decoder_fc3(h4))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Instantiate and check for CUDA\n",
    "vae = VAE(input_dim=784, hidden_dim1=512, hidden_dim2=256, latent_dim=2)\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc31): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (fc32): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (fc4): Linear(in_features=2, out_features=256, bias=True)\n",
       "  (fc5): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (fc6): Linear(in_features=512, out_features=784, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.Adam(vae.parameters())\n",
    "\n",
    "# Loss function: reconstruction error + KL divergence\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item() / len(data):.6f}')\n",
    "    print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    vae.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.cuda()\n",
    "            recon, mu, logvar = vae(data)\n",
    "            test_loss += loss_function(recon, data, mu, logvar).item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'====> Test set loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 543.505273\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 189.396211\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 177.549082\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 170.036738\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 160.531416\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 167.588437\n",
      "====> Epoch: 1 Average loss: 177.2862\n",
      "====> Test set loss: 161.2505\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 158.884492\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 161.030801\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 167.027832\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 151.838857\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 154.238750\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 155.915732\n",
      "====> Epoch: 2 Average loss: 157.1256\n",
      "====> Test set loss: 154.0658\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 155.922598\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 143.489014\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 152.752100\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 144.876035\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 158.972744\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 148.415059\n",
      "====> Epoch: 3 Average loss: 152.2611\n",
      "====> Test set loss: 150.4989\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 153.071973\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 152.632617\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 150.870039\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 156.205264\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 152.297979\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 147.780225\n",
      "====> Epoch: 4 Average loss: 149.3164\n",
      "====> Test set loss: 148.4869\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 152.309824\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 156.843311\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 139.443760\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 159.155898\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 154.303369\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 153.797861\n",
      "====> Epoch: 5 Average loss: 147.2951\n",
      "====> Test set loss: 146.9490\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 144.215303\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 138.242480\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 145.208613\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 146.256943\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 154.915527\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 150.248223\n",
      "====> Epoch: 6 Average loss: 145.8009\n",
      "====> Test set loss: 145.8192\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 141.927578\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 143.956250\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 142.306924\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 151.414990\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 140.794043\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 146.967012\n",
      "====> Epoch: 7 Average loss: 144.6945\n",
      "====> Test set loss: 145.2347\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 147.401338\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 140.251113\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 145.558975\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 135.367344\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 139.056162\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 144.377217\n",
      "====> Epoch: 8 Average loss: 143.8257\n",
      "====> Test set loss: 143.9981\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 150.180010\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 148.480479\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 140.537422\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 144.277061\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 135.739502\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 140.530859\n",
      "====> Epoch: 9 Average loss: 143.0860\n",
      "====> Test set loss: 144.4007\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 148.612500\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 137.011436\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 149.065039\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 137.450967\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 144.905703\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 141.446445\n",
      "====> Epoch: 10 Average loss: 142.3689\n",
      "====> Test set loss: 143.3964\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 138.407725\n",
      "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 144.936533\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 132.151211\n",
      "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 145.171562\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 144.641162\n",
      "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 145.143203\n",
      "====> Epoch: 11 Average loss: 141.8726\n",
      "====> Test set loss: 142.7006\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 148.474814\n",
      "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 141.825918\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 135.072520\n",
      "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 148.262441\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 149.130479\n",
      "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 144.610889\n",
      "====> Epoch: 12 Average loss: 141.2909\n",
      "====> Test set loss: 141.9970\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 143.247344\n",
      "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 141.053623\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 138.839443\n",
      "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 135.689531\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 142.268242\n",
      "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 136.899238\n",
      "====> Epoch: 13 Average loss: 140.9374\n",
      "====> Test set loss: 141.5393\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 140.469766\n",
      "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 128.901064\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 140.222451\n",
      "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 145.261924\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 134.531416\n",
      "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 141.395303\n",
      "====> Epoch: 14 Average loss: 140.4799\n",
      "====> Test set loss: 141.4247\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 150.690547\n",
      "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 129.463428\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 141.469756\n",
      "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 141.785937\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 140.621396\n",
      "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 148.604043\n",
      "====> Epoch: 15 Average loss: 140.0402\n",
      "====> Test set loss: 141.2530\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 133.194434\n",
      "Train Epoch: 16 [10000/60000 (17%)]\tLoss: 140.567842\n",
      "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 146.051855\n",
      "Train Epoch: 16 [30000/60000 (50%)]\tLoss: 135.386240\n",
      "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 144.936621\n",
      "Train Epoch: 16 [50000/60000 (83%)]\tLoss: 131.308701\n",
      "====> Epoch: 16 Average loss: 139.8015\n",
      "====> Test set loss: 141.3210\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 134.587480\n",
      "Train Epoch: 17 [10000/60000 (17%)]\tLoss: 138.646133\n",
      "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 131.309121\n",
      "Train Epoch: 17 [30000/60000 (50%)]\tLoss: 141.333916\n",
      "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 137.244883\n",
      "Train Epoch: 17 [50000/60000 (83%)]\tLoss: 130.518008\n",
      "====> Epoch: 17 Average loss: 139.4748\n",
      "====> Test set loss: 140.6263\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 139.360371\n",
      "Train Epoch: 18 [10000/60000 (17%)]\tLoss: 135.672539\n",
      "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 138.359814\n",
      "Train Epoch: 18 [30000/60000 (50%)]\tLoss: 139.727979\n",
      "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 136.856270\n",
      "Train Epoch: 18 [50000/60000 (83%)]\tLoss: 136.863926\n",
      "====> Epoch: 18 Average loss: 139.1225\n",
      "====> Test set loss: 140.5944\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 134.268965\n",
      "Train Epoch: 19 [10000/60000 (17%)]\tLoss: 144.395527\n",
      "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 138.951348\n",
      "Train Epoch: 19 [30000/60000 (50%)]\tLoss: 148.395664\n",
      "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 133.456299\n",
      "Train Epoch: 19 [50000/60000 (83%)]\tLoss: 138.415254\n",
      "====> Epoch: 19 Average loss: 138.7911\n",
      "====> Test set loss: 140.6384\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 134.040303\n",
      "Train Epoch: 20 [10000/60000 (17%)]\tLoss: 140.313115\n",
      "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 149.419531\n",
      "Train Epoch: 20 [30000/60000 (50%)]\tLoss: 139.246084\n",
      "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 141.305654\n",
      "Train Epoch: 20 [50000/60000 (83%)]\tLoss: 135.533477\n",
      "====> Epoch: 20 Average loss: 138.4856\n",
      "====> Test set loss: 139.7265\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 135.721045\n",
      "Train Epoch: 21 [10000/60000 (17%)]\tLoss: 136.613223\n",
      "Train Epoch: 21 [20000/60000 (33%)]\tLoss: 147.858203\n",
      "Train Epoch: 21 [30000/60000 (50%)]\tLoss: 141.417139\n",
      "Train Epoch: 21 [40000/60000 (67%)]\tLoss: 132.330498\n",
      "Train Epoch: 21 [50000/60000 (83%)]\tLoss: 131.857705\n",
      "====> Epoch: 21 Average loss: 138.0820\n",
      "====> Test set loss: 139.6980\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 134.316641\n",
      "Train Epoch: 22 [10000/60000 (17%)]\tLoss: 135.840527\n",
      "Train Epoch: 22 [20000/60000 (33%)]\tLoss: 135.431338\n",
      "Train Epoch: 22 [30000/60000 (50%)]\tLoss: 141.774385\n",
      "Train Epoch: 22 [40000/60000 (67%)]\tLoss: 129.830029\n",
      "Train Epoch: 22 [50000/60000 (83%)]\tLoss: 135.976387\n",
      "====> Epoch: 22 Average loss: 137.8209\n",
      "====> Test set loss: 139.4707\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 131.652012\n",
      "Train Epoch: 23 [10000/60000 (17%)]\tLoss: 140.275000\n",
      "Train Epoch: 23 [20000/60000 (33%)]\tLoss: 138.014990\n",
      "Train Epoch: 23 [30000/60000 (50%)]\tLoss: 130.384795\n",
      "Train Epoch: 23 [40000/60000 (67%)]\tLoss: 140.947275\n",
      "Train Epoch: 23 [50000/60000 (83%)]\tLoss: 137.425137\n",
      "====> Epoch: 23 Average loss: 137.7605\n",
      "====> Test set loss: 139.2240\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 138.698301\n",
      "Train Epoch: 24 [10000/60000 (17%)]\tLoss: 135.895137\n",
      "Train Epoch: 24 [20000/60000 (33%)]\tLoss: 139.415391\n",
      "Train Epoch: 24 [30000/60000 (50%)]\tLoss: 141.533359\n",
      "Train Epoch: 24 [40000/60000 (67%)]\tLoss: 140.097969\n",
      "Train Epoch: 24 [50000/60000 (83%)]\tLoss: 142.196230\n",
      "====> Epoch: 24 Average loss: 137.6268\n",
      "====> Test set loss: 139.2025\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 130.557266\n",
      "Train Epoch: 25 [10000/60000 (17%)]\tLoss: 139.329170\n",
      "Train Epoch: 25 [20000/60000 (33%)]\tLoss: 141.030596\n",
      "Train Epoch: 25 [30000/60000 (50%)]\tLoss: 134.847295\n",
      "Train Epoch: 25 [40000/60000 (67%)]\tLoss: 135.926006\n",
      "Train Epoch: 25 [50000/60000 (83%)]\tLoss: 145.211338\n",
      "====> Epoch: 25 Average loss: 137.3096\n",
      "====> Test set loss: 139.3084\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 139.050264\n",
      "Train Epoch: 26 [10000/60000 (17%)]\tLoss: 139.645234\n",
      "Train Epoch: 26 [20000/60000 (33%)]\tLoss: 139.121953\n",
      "Train Epoch: 26 [30000/60000 (50%)]\tLoss: 138.409355\n",
      "Train Epoch: 26 [40000/60000 (67%)]\tLoss: 145.367031\n",
      "Train Epoch: 26 [50000/60000 (83%)]\tLoss: 133.835977\n",
      "====> Epoch: 26 Average loss: 137.2013\n",
      "====> Test set loss: 138.8972\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 139.403613\n",
      "Train Epoch: 27 [10000/60000 (17%)]\tLoss: 138.158809\n",
      "Train Epoch: 27 [20000/60000 (33%)]\tLoss: 137.845234\n",
      "Train Epoch: 27 [30000/60000 (50%)]\tLoss: 134.479512\n",
      "Train Epoch: 27 [40000/60000 (67%)]\tLoss: 133.882041\n",
      "Train Epoch: 27 [50000/60000 (83%)]\tLoss: 134.356143\n",
      "====> Epoch: 27 Average loss: 137.0121\n",
      "====> Test set loss: 139.0942\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 141.897656\n",
      "Train Epoch: 28 [10000/60000 (17%)]\tLoss: 135.942939\n",
      "Train Epoch: 28 [20000/60000 (33%)]\tLoss: 141.133359\n",
      "Train Epoch: 28 [30000/60000 (50%)]\tLoss: 144.547119\n",
      "Train Epoch: 28 [40000/60000 (67%)]\tLoss: 141.622227\n",
      "Train Epoch: 28 [50000/60000 (83%)]\tLoss: 140.993096\n",
      "====> Epoch: 28 Average loss: 136.8414\n",
      "====> Test set loss: 138.6555\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 140.619297\n",
      "Train Epoch: 29 [10000/60000 (17%)]\tLoss: 133.696689\n",
      "Train Epoch: 29 [20000/60000 (33%)]\tLoss: 135.012998\n",
      "Train Epoch: 29 [30000/60000 (50%)]\tLoss: 138.802764\n",
      "Train Epoch: 29 [40000/60000 (67%)]\tLoss: 135.794639\n",
      "Train Epoch: 29 [50000/60000 (83%)]\tLoss: 134.930684\n",
      "====> Epoch: 29 Average loss: 136.5570\n",
      "====> Test set loss: 138.8395\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 140.287187\n",
      "Train Epoch: 30 [10000/60000 (17%)]\tLoss: 142.905059\n",
      "Train Epoch: 30 [20000/60000 (33%)]\tLoss: 133.474600\n",
      "Train Epoch: 30 [30000/60000 (50%)]\tLoss: 134.209072\n",
      "Train Epoch: 30 [40000/60000 (67%)]\tLoss: 137.300186\n",
      "Train Epoch: 30 [50000/60000 (83%)]\tLoss: 133.086250\n",
      "====> Epoch: 30 Average loss: 136.3781\n",
      "====> Test set loss: 138.6485\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 138.760713\n",
      "Train Epoch: 31 [10000/60000 (17%)]\tLoss: 131.771836\n",
      "Train Epoch: 31 [20000/60000 (33%)]\tLoss: 135.782969\n",
      "Train Epoch: 31 [30000/60000 (50%)]\tLoss: 131.106787\n",
      "Train Epoch: 31 [40000/60000 (67%)]\tLoss: 134.905996\n",
      "Train Epoch: 31 [50000/60000 (83%)]\tLoss: 137.494180\n",
      "====> Epoch: 31 Average loss: 136.2961\n",
      "====> Test set loss: 138.5996\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 136.453301\n",
      "Train Epoch: 32 [10000/60000 (17%)]\tLoss: 137.960078\n",
      "Train Epoch: 32 [20000/60000 (33%)]\tLoss: 135.558799\n",
      "Train Epoch: 32 [30000/60000 (50%)]\tLoss: 137.303691\n",
      "Train Epoch: 32 [40000/60000 (67%)]\tLoss: 144.780059\n",
      "Train Epoch: 32 [50000/60000 (83%)]\tLoss: 144.248301\n",
      "====> Epoch: 32 Average loss: 136.1913\n",
      "====> Test set loss: 138.3732\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 138.724102\n",
      "Train Epoch: 33 [10000/60000 (17%)]\tLoss: 133.507285\n",
      "Train Epoch: 33 [20000/60000 (33%)]\tLoss: 132.943096\n",
      "Train Epoch: 33 [30000/60000 (50%)]\tLoss: 136.839980\n",
      "Train Epoch: 33 [40000/60000 (67%)]\tLoss: 135.307695\n",
      "Train Epoch: 33 [50000/60000 (83%)]\tLoss: 132.671641\n",
      "====> Epoch: 33 Average loss: 136.1849\n",
      "====> Test set loss: 138.8449\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 128.114453\n",
      "Train Epoch: 34 [10000/60000 (17%)]\tLoss: 132.496602\n",
      "Train Epoch: 34 [20000/60000 (33%)]\tLoss: 136.191406\n",
      "Train Epoch: 34 [30000/60000 (50%)]\tLoss: 134.410313\n",
      "Train Epoch: 34 [40000/60000 (67%)]\tLoss: 135.834385\n",
      "Train Epoch: 34 [50000/60000 (83%)]\tLoss: 138.898262\n",
      "====> Epoch: 34 Average loss: 136.2024\n",
      "====> Test set loss: 138.3444\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 128.617168\n",
      "Train Epoch: 35 [10000/60000 (17%)]\tLoss: 134.166826\n",
      "Train Epoch: 35 [20000/60000 (33%)]\tLoss: 138.337617\n",
      "Train Epoch: 35 [30000/60000 (50%)]\tLoss: 128.583447\n",
      "Train Epoch: 35 [40000/60000 (67%)]\tLoss: 133.765713\n",
      "Train Epoch: 35 [50000/60000 (83%)]\tLoss: 137.820742\n",
      "====> Epoch: 35 Average loss: 136.1006\n",
      "====> Test set loss: 138.2550\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 123.548457\n",
      "Train Epoch: 36 [10000/60000 (17%)]\tLoss: 126.008809\n",
      "Train Epoch: 36 [20000/60000 (33%)]\tLoss: 132.826152\n",
      "Train Epoch: 36 [30000/60000 (50%)]\tLoss: 137.243701\n",
      "Train Epoch: 36 [40000/60000 (67%)]\tLoss: 136.010361\n",
      "Train Epoch: 36 [50000/60000 (83%)]\tLoss: 135.879658\n",
      "====> Epoch: 36 Average loss: 136.1234\n",
      "====> Test set loss: 138.6677\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 138.337764\n",
      "Train Epoch: 37 [10000/60000 (17%)]\tLoss: 140.271689\n",
      "Train Epoch: 37 [20000/60000 (33%)]\tLoss: 134.311621\n",
      "Train Epoch: 37 [30000/60000 (50%)]\tLoss: 131.781611\n",
      "Train Epoch: 37 [40000/60000 (67%)]\tLoss: 131.368330\n",
      "Train Epoch: 37 [50000/60000 (83%)]\tLoss: 130.606338\n",
      "====> Epoch: 37 Average loss: 135.8411\n",
      "====> Test set loss: 138.2784\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 133.764395\n",
      "Train Epoch: 38 [10000/60000 (17%)]\tLoss: 131.075479\n",
      "Train Epoch: 38 [20000/60000 (33%)]\tLoss: 134.908330\n",
      "Train Epoch: 38 [30000/60000 (50%)]\tLoss: 130.063340\n",
      "Train Epoch: 38 [40000/60000 (67%)]\tLoss: 141.632090\n",
      "Train Epoch: 38 [50000/60000 (83%)]\tLoss: 134.455381\n",
      "====> Epoch: 38 Average loss: 135.6535\n",
      "====> Test set loss: 138.1224\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 135.449248\n",
      "Train Epoch: 39 [10000/60000 (17%)]\tLoss: 133.263975\n",
      "Train Epoch: 39 [20000/60000 (33%)]\tLoss: 129.326748\n",
      "Train Epoch: 39 [30000/60000 (50%)]\tLoss: 126.316260\n",
      "Train Epoch: 39 [40000/60000 (67%)]\tLoss: 141.431543\n",
      "Train Epoch: 39 [50000/60000 (83%)]\tLoss: 142.661982\n",
      "====> Epoch: 39 Average loss: 135.3235\n",
      "====> Test set loss: 138.1556\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 134.481611\n",
      "Train Epoch: 40 [10000/60000 (17%)]\tLoss: 135.481514\n",
      "Train Epoch: 40 [20000/60000 (33%)]\tLoss: 137.353145\n",
      "Train Epoch: 40 [30000/60000 (50%)]\tLoss: 136.084316\n",
      "Train Epoch: 40 [40000/60000 (67%)]\tLoss: 139.435273\n",
      "Train Epoch: 40 [50000/60000 (83%)]\tLoss: 139.281240\n",
      "====> Epoch: 40 Average loss: 135.3498\n",
      "====> Test set loss: 137.9632\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 139.020449\n",
      "Train Epoch: 41 [10000/60000 (17%)]\tLoss: 128.751201\n",
      "Train Epoch: 41 [20000/60000 (33%)]\tLoss: 141.886016\n",
      "Train Epoch: 41 [30000/60000 (50%)]\tLoss: 141.310410\n",
      "Train Epoch: 41 [40000/60000 (67%)]\tLoss: 136.497578\n",
      "Train Epoch: 41 [50000/60000 (83%)]\tLoss: 138.988584\n",
      "====> Epoch: 41 Average loss: 135.2469\n",
      "====> Test set loss: 138.2809\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 134.171904\n",
      "Train Epoch: 42 [10000/60000 (17%)]\tLoss: 136.487695\n",
      "Train Epoch: 42 [20000/60000 (33%)]\tLoss: 133.063848\n",
      "Train Epoch: 42 [30000/60000 (50%)]\tLoss: 132.734297\n",
      "Train Epoch: 42 [40000/60000 (67%)]\tLoss: 134.323740\n",
      "Train Epoch: 42 [50000/60000 (83%)]\tLoss: 129.972861\n",
      "====> Epoch: 42 Average loss: 135.0807\n",
      "====> Test set loss: 138.0268\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 138.790166\n",
      "Train Epoch: 43 [10000/60000 (17%)]\tLoss: 134.363818\n",
      "Train Epoch: 43 [20000/60000 (33%)]\tLoss: 132.159453\n",
      "Train Epoch: 43 [30000/60000 (50%)]\tLoss: 131.874326\n",
      "Train Epoch: 43 [40000/60000 (67%)]\tLoss: 120.424102\n",
      "Train Epoch: 43 [50000/60000 (83%)]\tLoss: 133.465928\n",
      "====> Epoch: 43 Average loss: 135.0448\n",
      "====> Test set loss: 138.1380\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 128.715420\n",
      "Train Epoch: 44 [10000/60000 (17%)]\tLoss: 133.537070\n",
      "Train Epoch: 44 [20000/60000 (33%)]\tLoss: 131.329238\n",
      "Train Epoch: 44 [30000/60000 (50%)]\tLoss: 132.630801\n",
      "Train Epoch: 44 [40000/60000 (67%)]\tLoss: 130.982412\n",
      "Train Epoch: 44 [50000/60000 (83%)]\tLoss: 138.151201\n",
      "====> Epoch: 44 Average loss: 135.1876\n",
      "====> Test set loss: 137.8655\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 133.560000\n",
      "Train Epoch: 45 [10000/60000 (17%)]\tLoss: 139.918262\n",
      "Train Epoch: 45 [20000/60000 (33%)]\tLoss: 132.207568\n",
      "Train Epoch: 45 [30000/60000 (50%)]\tLoss: 134.795879\n",
      "Train Epoch: 45 [40000/60000 (67%)]\tLoss: 135.329951\n",
      "Train Epoch: 45 [50000/60000 (83%)]\tLoss: 130.857510\n",
      "====> Epoch: 45 Average loss: 134.8518\n",
      "====> Test set loss: 137.8690\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 138.156689\n",
      "Train Epoch: 46 [10000/60000 (17%)]\tLoss: 128.611680\n",
      "Train Epoch: 46 [20000/60000 (33%)]\tLoss: 138.557559\n",
      "Train Epoch: 46 [30000/60000 (50%)]\tLoss: 135.156113\n",
      "Train Epoch: 46 [40000/60000 (67%)]\tLoss: 131.649883\n",
      "Train Epoch: 46 [50000/60000 (83%)]\tLoss: 140.606982\n",
      "====> Epoch: 46 Average loss: 134.9226\n",
      "====> Test set loss: 137.7145\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 132.324668\n",
      "Train Epoch: 47 [10000/60000 (17%)]\tLoss: 139.922607\n",
      "Train Epoch: 47 [20000/60000 (33%)]\tLoss: 141.653418\n",
      "Train Epoch: 47 [30000/60000 (50%)]\tLoss: 131.448848\n",
      "Train Epoch: 47 [40000/60000 (67%)]\tLoss: 133.542295\n",
      "Train Epoch: 47 [50000/60000 (83%)]\tLoss: 137.359385\n",
      "====> Epoch: 47 Average loss: 134.8984\n",
      "====> Test set loss: 137.6443\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 140.089150\n",
      "Train Epoch: 48 [10000/60000 (17%)]\tLoss: 142.579746\n",
      "Train Epoch: 48 [20000/60000 (33%)]\tLoss: 128.939277\n",
      "Train Epoch: 48 [30000/60000 (50%)]\tLoss: 142.300566\n",
      "Train Epoch: 48 [40000/60000 (67%)]\tLoss: 128.115078\n",
      "Train Epoch: 48 [50000/60000 (83%)]\tLoss: 138.783477\n",
      "====> Epoch: 48 Average loss: 134.6844\n",
      "====> Test set loss: 137.5847\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 130.413604\n",
      "Train Epoch: 49 [10000/60000 (17%)]\tLoss: 132.940928\n",
      "Train Epoch: 49 [20000/60000 (33%)]\tLoss: 132.059668\n",
      "Train Epoch: 49 [30000/60000 (50%)]\tLoss: 133.525674\n",
      "Train Epoch: 49 [40000/60000 (67%)]\tLoss: 140.610869\n",
      "Train Epoch: 49 [50000/60000 (83%)]\tLoss: 132.882402\n",
      "====> Epoch: 49 Average loss: 134.8562\n",
      "====> Test set loss: 138.1535\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 130.257588\n",
      "Train Epoch: 50 [10000/60000 (17%)]\tLoss: 140.512676\n",
      "Train Epoch: 50 [20000/60000 (33%)]\tLoss: 133.809727\n",
      "Train Epoch: 50 [30000/60000 (50%)]\tLoss: 134.011318\n",
      "Train Epoch: 50 [40000/60000 (67%)]\tLoss: 135.890273\n",
      "Train Epoch: 50 [50000/60000 (83%)]\tLoss: 134.381113\n",
      "====> Epoch: 50 Average loss: 134.6331\n",
      "====> Test set loss: 137.8401\n"
     ]
    }
   ],
   "source": [
    "# Training and testing loop\n",
    "for epoch in range(1, 51):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 2\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     sample \u001b[38;5;241m=\u001b[39m vae\u001b[38;5;241m.\u001b[39mdecoder(z)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      5\u001b[0m     save_image(sample\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./samples/sample_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    z = torch.randn(64, 2).cuda() if torch.cuda.is_available() else torch.randn(64, 2)\n",
    "    sample = vae.decode(z)\n",
    "    \n",
    "    save_image(sample.view(64, 1, 28, 28), './samples/sample.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
